{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models to Predict March Madness Rankings\n",
    "\n",
    "\n",
    "March Madness, also known as the NCAA Division I Men's Basketball Tournament happens annually in the month of March. Depending on which teams performed the best in the season, the top 32 are selected to compete in the tournament and play each other in a bracket for the winners trophy. Although March Madness 2018 is already over, our team wanted to see which model would do a better job at predicting team rankings/winners. We decided to look at the Elo Model as well as use linear regression with features we extracted in order to find a trend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the Data\n",
    "\n",
    "In order to scrape data, we used www.sports-reference.com/cbb (cbb = college basketball). We utilized the BeautifulSoup library to extract the features that we thought we would need for both models. In the following code the features extracted are in the _featuresWanted_ set. A typical page that we would scrape from looks like the following: \n",
    "\n",
    "\n",
    "<img src=\"files/cbbstatsex.png\">\n",
    "\n",
    "This data displays Villanova's game history for the year 2018 [found here](https://www.sports-reference.com/cbb/schools/villanova/2018-schedule.html). We used Beautiful Soup to gather all the table data and format it in a data frame. Because the scraping usually takes ~10 minutes, the code was run once and put into a csv file, which we later used to do our data analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSchools():\n",
    "    url = \"https://www.sports-reference.com/cbb/seasons/2018-school-stats.html\"\n",
    "    page = urlopen(url).read()\n",
    "    soup = BeautifulSoup(page)\n",
    "    count  = 0\n",
    "    table = soup.find(\"tbody\")\n",
    "    school_dict = dict()\n",
    "    for row in table.findAll('td', {\"data-stat\": \"school_name\"}):\n",
    "        school_name = row.getText()\n",
    "        for a in row.find_all('a', href=True):\n",
    "            link = a['href'].strip()\n",
    "            name = link[13:].split(\"/\")[0]\n",
    "            school_dict[name] = school_name\n",
    "            \n",
    "    return school_dict\n",
    "\n",
    "def getDfs():\n",
    "    school_set = getSchools()\n",
    "    dfs = []\n",
    "    final_df=pd.DataFrame()\n",
    "    for school in school_set: \n",
    "        url = \"https://www.sports-reference.com/cbb/schools/\" + school + \"/2018-schedule.html\"\n",
    "        page = urlopen(url).read()\n",
    "        soup = BeautifulSoup(page)\n",
    "        count = 0 \n",
    "        pre_df = dict()\n",
    "        school_set = getSchools()\n",
    "        table = soup.find(\"tbody\")\n",
    "        featuresWanted =  {'opp_name', 'pts', 'opp_pts', \n",
    "                           'game_location','game_result','overtimes','wins','losses', 'date_game'} #add more features here!!\n",
    "\n",
    "        rows = table.findChildren(['tr'])\n",
    "        for row in rows:\n",
    "            if (row.find('th', {\"scope\":\"row\"}) != None):\n",
    "\n",
    "                for f in featuresWanted:\n",
    "                    cell = row.find(\"td\",{\"data-stat\": f})\n",
    "\n",
    "                    a = cell.text.strip().encode()\n",
    "                    text=a.decode(\"utf-8\")\n",
    "                    if f in pre_df:\n",
    "                        pre_df[f].append(text)\n",
    "                    else:\n",
    "                        pre_df[f]=[text]\n",
    "            \n",
    "        df = pd.DataFrame.from_dict(pre_df)\n",
    "        df[\"opp_name\"]= df[\"opp_name\"].apply(lambda row: (row.split(\"(\")[0]).rstrip())\n",
    "        df[\"school_name\"]=school_set[school]\n",
    "        df[\"school_name\"] = df[\"school_name\"].apply(lambda row: (row.split(\"(\")[0]).rstrip())\n",
    "        final_df=pd.concat([final_df,df])\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def csvDump():\n",
    "    df=getDfs()\n",
    "    df.to_csv(\"scraped_data.csv\")\n",
    "    \n",
    "    \n",
    "csvDump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the csv, our csv (in this same folder, called scraped_data.csv contained data about all games that were played in the 2017-2018 season."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Elo Model \n",
    "\n",
    "The Elo Model is a way of creating a rating system for zero-sum games - games that only have one winner and one loser (e.g. basketball, hockey, football, tennis, etc.) The system uses the following method:\n",
    "\n",
    "The algorithm works in the following way: \n",
    "\n",
    "Each team begins with the same ranking. The standard across most sports is ~1000-1500. We started out with *1200*, which was a common trend amongst others across the internet who had also used Elo Rankings for other sports. We then calculate the probability of each team winning with the following equation:\n",
    "\n",
    "**Team1 Probability = (1.0 / (1.0 + 10^((Team1_Rating – Team2_Rating) / 400)))**\n",
    "\n",
    "**Team2 Probability  = (1.0 / (1.0 + 10^((Team2_Rating – Team1_Rating) / 400)))**\n",
    "\n",
    "\n",
    "We can see that Team1 Probabilty + Team2 Probabiilty = 1.0. The '400' is a standardized constant in Elo Rankings[(1)](https://en.wikipedia.org/wiki/Elo_rating_system)\n",
    "\n",
    "When a game is played, we can update the rankings of both teams using the following equation: \n",
    "\n",
    "**Team1_Rating = Team1_Rating + K*(Team1_Score – Team1_Probability)**\n",
    "\n",
    "**Team2_Rating = Team2_Rating + K*(Team2_Score – Team2_Probability)**\n",
    "\n",
    "Here, the scores are determined by the outcome of the game:\n",
    "\n",
    "win = 1.0\n",
    "draw = 0.5\n",
    "loss = 0.0\n",
    "\n",
    "The K factor is a numerical value that \"determines how much the Elo rating should change following a match result\"[(2)](www.betfair.com.au). Across literature and the internet, a common k-factor for basketball has been 20 (Used by FiveThirtyEight and others). We can actually create a K=factor that depeonds on the nubmer of matches played. (More on this later). \n",
    "\n",
    "\n",
    "The following Elo class creates an Elo ranking for each team and updates it everytime a game is played. It will be used for data analysis on the data we scraped earlier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "WIN = 1.\n",
    "DRAW = 0.5\n",
    "LOSS = 0.\n",
    "\n",
    "https://www.geeksforgeeks.org/elo-rating-algorithm/\n",
    "\n",
    "'''\n",
    "#: Default K-factor.\n",
    "K_FACTOR = 25\n",
    "#: Default rating class.\n",
    "RATING_CLASS = float\n",
    "#: Default initial rating.\n",
    "INITIAL_RATING = 1200\n",
    "#: Default Beta value.\n",
    "BETA = 200\n",
    "\n",
    "\n",
    "class Elo(object):\n",
    "    #initialize object\n",
    "    def __init__(self, teamName, kFactor = K_FACTOR, rating = INITIAL_RATING, beta = BETA):\n",
    "        self.teamName = teamName\n",
    "        self.kFactor = kFactor\n",
    "        self.rating = rating \n",
    "        self.pWin = None\n",
    "        self.beta = 2*BETA\n",
    "        self.matches = 0 \n",
    "\n",
    "    def calcPWin(self, oppRating): #expected\n",
    "        pwin = 1/(1+1000.00**((self.rating - oppRating)/self.beta))\n",
    "        self.pWin = pwin\n",
    "        return pwin\n",
    "\n",
    "    def game(self, outcome, oppRating): #1 for win, 0 for loss, \n",
    "        pwin =self.calcPWin(oppRating)\n",
    "        self.rating = self.rating - self.kFactor*(outcome - pwin)\n",
    "        self.matches+=1\n",
    "        return True\n",
    "\n",
    "    def getPWin(self):\n",
    "        return self.pWin\n",
    "\n",
    "    def getRating(self):\n",
    "        return self.rating\n",
    "\n",
    "    def setKFactor(self, k):\n",
    "        self.kFactor = k "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the ELOs ## \n",
    "\n",
    "We computed the ELOs for every team by instantiating ELO class objects for all the major NCAA schools in the main function. Then we iterate through the data frame of games that we scraped, and updated the ELO value for all of the major schools for all regular season games. After sorting based on the ELO values, we updated the rankings and used this dictionary as our predicted outcomes of the March Madness tournament. \n",
    "\n",
    "\n",
    "The following includes high level steps of the main function: \n",
    "\t1. We first clean the school name to remove, necessary characters\n",
    "\t2. Iterate through every game\n",
    "        a. Update the home and opp team based their respective team ratings and their W/Ls \n",
    "\t4. After we have the final ELOs, we sort the list based on their ELO ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def removeNCAA(x):\n",
    "    #removes unnecessary characters\n",
    "    if(\"NCAA\" in x):\n",
    "        return x[:-5]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def ranking(schoolDictionary): \n",
    "    outputs = []\n",
    "    for key in schoolDictionary: \n",
    "        eloObject = schoolDictionary[key]\n",
    "        item = (eloObject.teamName, eloObject.rating)\n",
    "        outputs.append(item)\n",
    "    return sorted(outputs,  key=lambda x: x[1])\n",
    "     \n",
    "    \n",
    "def runELO(): \n",
    "    df = pd.read_csv(\"scraped_data.csv\")\n",
    "    df.drop(['Unnamed: 0'], axis = 1, inplace=True)\n",
    "   # df['date_game'] =pd.to_datetime(df.date_game)\n",
    "    df[\"school_name\"].apply(removeNCAA)\n",
    "    schoolDict = {} \n",
    "    schools = set(df['school_name'])\n",
    "\n",
    "\n",
    "    for school in schools: \n",
    "        if school not in schoolDict: \n",
    "            schoolDict[school] = Elo(school)\n",
    "    for index, row in df.iterrows(): \n",
    "        homeSchool = row[\"school_name\"]\n",
    "        oppSchool = row[\"opp_name\"]\n",
    "        if oppSchool not in schoolDict:\n",
    "            #even if the opponent was not in the schools dictionary, then the opponent is not a major school\n",
    "            #assigned this team a rating of 100, so that this win is not impact our ELO rating as a much as a major school would \n",
    "            oppRating = 100\n",
    "            oppObj = None \n",
    "        else: \n",
    "            oppObj = schoolDict[oppSchool]\n",
    "            oppRating = oppObj.getRating()\n",
    "        #getting the rating before we update with the outcome, so that we can use the before game ELO to correctly \n",
    "        #reflect the outcome for the opponent after changing home team win\n",
    "        \n",
    "        schoolObj = schoolDict[homeSchool]\n",
    "        schoolRating = schoolObj.getRating() \n",
    "    \n",
    "        result = row[\"game_result\"]\n",
    "        if result == 'W': \n",
    "            schoolObj.game(1, oppRating)\n",
    "            if oppObj != None: \n",
    "                oppObj.game(0, schoolRating)\n",
    "        else: \n",
    "            schoolObj.game(0, oppRating)\n",
    "            if oppObj != None: \n",
    "                oppObj.game(1, schoolRating)\n",
    "        schoolDict[homeSchool] = schoolObj\n",
    "    ranks = ranking(schoolDict)\n",
    "    d = dict()\n",
    "    for i in range(len(ranks)):\n",
    "        d[ranks[i]]= i+1\n",
    "    return d\n",
    "        \n",
    "    \n",
    "d1 = runELO()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Quality of ELO ## \n",
    "\n",
    "We used the output from our runELO function as the ELO's predicted data and computed its MSE against the actual March Madness data which we got from the get_original_MM_rankings() function. To determine the quality of this model, we decided to compute the MSE between the Associated Press’s ranking predictions and the actual. We got Associated Press's ranking predictions by running get_original_AP_rankings(). \n",
    "\n",
    "We then determined the quality of the model by finding the mean squared errors between the predicted and actual rankings. The mean squared algorithm follows the following steps to determine how well the predicted rankings fit the actual rankings. \n",
    "\n",
    "    1. Go through the teams in predicted data  \n",
    "    2. Computed the difference between the actual and the predicted data, squared this value, and added it to a list \n",
    "    3. After all the squared errors have been computed, take the average of the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_original_MM_rankings():\n",
    "    MM = {1: \"Villanova\", 2:\"Michigan\", 3: \"Loyola (IL)\", 4: \"Kansas\", 5: \"Duke\", 6: \"Florida State\",\n",
    "         7: \"Kansas State\", 8: \"Texas Tech\", 9: \"Nevada\", 10: \"Kentucky\", 11: \"Clemson\", 12: \"Syracuse\",\n",
    "         13: \"Texas Tech\", 14: \"Gonzaga\", 15: \"West Virginia\", 16: \"Texas A&M\", 17:\"Houston\", 18: \"Tennessee\",19: \"Cincinnati\",\n",
    "        20: \"Michigan\",21: \"Butler\", 22: \"Florida\", 23: \"Seton Hall\",24: \"Xavier\",25: \"Ohio State\",\n",
    "          26: \"Maryland-Baltimore County\", 27: \"Rhode Island\",28: \"Buffalo\", 29: \"North Carolina\",30: \"Marshall\", 31: \"Alabama\",32:\"Auburn\" } \n",
    "\n",
    "    res = {val:key for (key, val) in MM.items()}\n",
    "    return res\n",
    "\n",
    "def get_original_AP_rankings():\n",
    "    AP = {1: \"Virginia\", 2:\"Villanova\", 3: \"Xavier\", 4: \"Kansas\", 5: \"Michigan State\", 6: \"Cincinnati\",\n",
    "         7: \"Michigan\", 8: \"Gonzaga\", 9: \"Duke\", 10: \"North Carolina\", 11: \"Purdue\", 12: \"Arizona\",\n",
    "         13: \"Tennessee\", 14: \"Texas Tech\", 15: \"West Virginia\", 16: \"Wichita State\", \n",
    "          17:\"Ohio State\", 18: \"Kentucky\",19: \"Auburn\",\n",
    "        20: \"Clemson\",21: \"Houston\", 22: \"Miami (FL)\", 23: \"Florida\",24: \"Nevada\",25: \"Saint Mary's (CA)\"}\n",
    "    {val:key for (key, val) in AP.items()}\n",
    "    return AP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mseELO(n):\n",
    "    #predicted data from the main function \n",
    "    predicted = {val:key for (key, val) in d1.items()}\n",
    "    #actual data from the dictionary returned \n",
    "    actual = get_original_MM_rankings()\n",
    "    ranks = []\n",
    "    for i in range(n):\n",
    "        rank = i+1 \n",
    "        (school,score) = predicted[rank]\n",
    "        school = (removeNCAA(school))\n",
    "        actualRankings = {val:key for (key, val) in actual.items()}\n",
    "        if school not in actual: \n",
    "            #since the school is not in the top 25 March Madness teams after the tournament \n",
    "            #assign the actual rank as n+1, which is the maximum (worst) rank \n",
    "            actualRank = n+1 \n",
    "        else: \n",
    "            actualRank = actual[school]        \n",
    "        rank_diff = abs(rank-actualRank)**2 \n",
    "        ranks.append(rank_diff)\n",
    "    mse = sum(ranks) / len(ranks)\n",
    "    return (mse) \n",
    "\n",
    "def mseAP(n):\n",
    "    predicted = get_original_AP_rankings()\n",
    "    actual = get_original_MM_rankings()\n",
    "\n",
    "    ranks = []\n",
    "    for i in range(n):\n",
    "        rank = i+1 \n",
    "        (school) = predicted[rank]\n",
    "        school = (removeNCAA(school))\n",
    "        actualRankings = {val:key for (key, val) in actual.items()}\n",
    "        if school not in actual: \n",
    "            #since the school is not in the top 25 March Madness teams after the tournament \n",
    "            #assign the actual rank as n+1, which is the maximum (worst) rank \n",
    "            actualRank = n+1\n",
    "        else: \n",
    "            actualRank = actual[school]        \n",
    "        rank_diff = abs(rank-actualRank)**2 \n",
    "        ranks.append(rank_diff)\n",
    "    mse = sum(ranks) / len(ranks)\n",
    "    return (mse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ELO's means squared error is:  110.36\n",
      "The AP's means squared error is:  137.72\n"
     ]
    }
   ],
   "source": [
    "print(\"The ELO's means squared error is: \", mseELO(25))\n",
    "print(\"The AP's means squared error is: \", mseAP(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: ELO ## \n",
    "As we can see from above, our ELO predictions had a better MSE than the AP's predictions, thus proving that our predictions fit the results better than the AP's predictions. We do see that this number is still quite high, but we will see that using a linear regression and training on certain features will produce better results than the ELO model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](elo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model 2: Linear Regression\n",
    "\n",
    "In practice with Machine Learning fitting a model to data is an iterative process. It takes multiple models and attempts to find a Machine Learning algorithm that achieves the desired performance. Therefore, in our project we decided to also implement a Linear Regression Model and compare the results produced with those from the ELO Rating.\n",
    "\n",
    "As we learned in class, Linear Regression is estimating the vector of parameters Beta, from the equation\n",
    "\n",
    "y=X*b+e. Where y is a vector of outputs, e is noise, and X is a design matrix. A design matrix consists of rows of examples where each row vector contains all the features for that example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use Linear Regression?\n",
    "\n",
    "Linear Regression is used when the outcome (dependent variable) is continuous and when your input features are\n",
    "continuous.\n",
    "\n",
    "#### Coefficient interpretation\n",
    "\n",
    "In linear regression, the coefficient interpretation of independent variables (features) is as follows:\n",
    "       \n",
    "           --->Holding all other variables constant, with a unit increase in this variable, the dependent variable \n",
    "           is expected to increase/decrease by this much.\n",
    "\n",
    "\n",
    "#### Error minimization technique\n",
    "\n",
    "Linear regression uses ordinary least squares method to minimize the errors and arrive at a best possible fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting March Madness Rankings through Linear Regression\n",
    "\n",
    "**Inputs** are continious integer or float values that represent things like number of wins, losses, and free point percentage \n",
    "\n",
    "**Output** is a rating per team denominated as points below/above average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data for Linear Regression \n",
    "\n",
    "We will be scraping our data from sports-reference.com.\n",
    "\n",
    "The features we identified as potentially significantly significant:\n",
    "\n",
    "    1.) Games Played\n",
    "    2.) Wins\n",
    "    3.) Losses\n",
    "    4.) Win-Loss %\n",
    "    5.) Points\n",
    "    6.) Field Goal%\n",
    "    7.) Free Throw%\n",
    "    8.) Offensive Rebounds\n",
    "    9.) Assists\n",
    "    10.) Steals\n",
    "    11.) Blocks\n",
    "    12.) Turnovers\n",
    "    13.) Personal Fouls\n",
    "\n",
    "For the 351 schools from the website, we must gather all of these features for each school and create an input feature matrix.\n",
    "\n",
    "We must gather 2 sets of X data. One from 2016-2017 season data to train on and one from 2017-2018 season data to predict on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2016-2017 Training Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Manini/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/Manini/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def getData():\n",
    "    url = \"https://www.sports-reference.com/cbb/seasons/2017-school-stats.html\"\n",
    "    page = urlopen(url).read()\n",
    "    soup = BeautifulSoup(page)\n",
    "    count  = 0\n",
    "    table = soup.find(\"tbody\")\n",
    "    featuresWanted =  {'school_name', 'g', 'wins', 'losses', \n",
    "                       'win_loss_pct', 'pts', 'fg_pct', 'fg3_pct', 'ft_pct', 'orb','trb', 'ast', 'stl', 'blk'\n",
    "                       , 'tov', 'pf'}\n",
    "    #find all rows-schools\n",
    "    rows = table.findChildren(['tr'])\n",
    "    pre_df={}\n",
    "    for row in rows:\n",
    "        if (row.find('th', {\"scope\":\"row\"}) != None):\n",
    "            #iterate through all features in set\n",
    "            for f in featuresWanted:\n",
    "                cell = row.find(\"td\",{\"data-stat\": f})\n",
    "                a = cell.text.strip().encode()\n",
    "                #remove any non-ascii characters\n",
    "                text=a.decode(\"utf-8\")\n",
    "                #append each school's data into a dictionary of features\n",
    "                if f in pre_df:\n",
    "                    pre_df[f].append(text)\n",
    "                else:\n",
    "                    pre_df[f]=[text]\n",
    "        #convert dictionary into a pandas dataframe\n",
    "        df = pd.DataFrame.from_dict(pre_df)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def csvDump():\n",
    "    df=getData()\n",
    "    df.to_csv(\"scraped_data_lr.csv\")\n",
    "    \n",
    "csvDump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2017-2018 Testing Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Manini/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/Manini/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "def getData2():\n",
    "    url = \"https://www.sports-reference.com/cbb/seasons/2018-school-stats.html\"\n",
    "    page = urlopen(url).read()\n",
    "    soup = BeautifulSoup(page)\n",
    "    count  = 0\n",
    "    table = soup.find(\"tbody\")\n",
    "    featuresWanted =  {'school_name', 'g', 'wins', 'losses', \n",
    "                       'win_loss_pct', 'pts', 'fg_pct', 'fg3_pct', 'ft_pct', 'orb','trb', 'ast', 'stl', 'blk'\n",
    "                       , 'tov', 'pf'}\n",
    "    rows = table.findChildren(['tr'])\n",
    "    pre_df={}\n",
    "    #find all rows-schools\n",
    "    for row in rows:\n",
    "        if (row.find('th', {\"scope\":\"row\"}) != None):\n",
    "            #iterate through all features in set\n",
    "            for f in featuresWanted:\n",
    "                cell = row.find(\"td\",{\"data-stat\": f})\n",
    "                a = cell.text.strip().encode()\n",
    "                #remove any non-ascii characters\n",
    "                text=a.decode(\"utf-8\")\n",
    "                #append each school's data into a dictionary of features\n",
    "                if f in pre_df:\n",
    "                    pre_df[f].append(text)\n",
    "                else:\n",
    "                    pre_df[f]=[text]\n",
    "        #convert dictionary into a pandas dataframe\n",
    "        df = pd.DataFrame.from_dict(pre_df)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def csvDump2():\n",
    "    df=getData2()\n",
    "    df.to_csv(\"scraped_data_lr_test_X.csv\")\n",
    "    \n",
    "csvDump2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering and Data Validation\n",
    "\n",
    "We also want to feature engineer our data according to the following guidelines to ensure that the data is in the correct format for the Linear Regression Model\n",
    "\n",
    "**1.)** Linear models typically have a constant bias term. We will encode this as a column of 1s in the dataframe. Call this column 'bias'.\n",
    "\n",
    "**2.)** We will keep most of the columns as is, since they are already numerical\n",
    "\n",
    "**3.)** Normalize Data\n",
    "\n",
    "**4.)** Check types of all data: \n",
    "\n",
    "    ast, blk, g, losses, orb, pf, pts,stl, tov, trb, wins (INTS)\n",
    "   \n",
    "    fg3_pct, fg_pct, ft_pct, win_loss_pct (FLOAT)\n",
    "   \n",
    "After scraping the data we want to ensure at the very least, that all of the schools data is present in our feature matrix. We can easily do this by checking the dimensions of our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ast</th>\n",
       "      <th>blk</th>\n",
       "      <th>fg3_pct</th>\n",
       "      <th>fg_pct</th>\n",
       "      <th>ft_pct</th>\n",
       "      <th>g</th>\n",
       "      <th>losses</th>\n",
       "      <th>orb</th>\n",
       "      <th>pf</th>\n",
       "      <th>pts</th>\n",
       "      <th>stl</th>\n",
       "      <th>tov</th>\n",
       "      <th>trb</th>\n",
       "      <th>win_loss_pct</th>\n",
       "      <th>wins</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>408</td>\n",
       "      <td>83</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.663</td>\n",
       "      <td>29</td>\n",
       "      <td>16</td>\n",
       "      <td>222</td>\n",
       "      <td>631</td>\n",
       "      <td>2009</td>\n",
       "      <td>210</td>\n",
       "      <td>412</td>\n",
       "      <td>904</td>\n",
       "      <td>0.448</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>494</td>\n",
       "      <td>57</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.717</td>\n",
       "      <td>33</td>\n",
       "      <td>21</td>\n",
       "      <td>304</td>\n",
       "      <td>594</td>\n",
       "      <td>2411</td>\n",
       "      <td>197</td>\n",
       "      <td>390</td>\n",
       "      <td>1149</td>\n",
       "      <td>0.364</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>517</td>\n",
       "      <td>118</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.683</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>357</td>\n",
       "      <td>653</td>\n",
       "      <td>2772</td>\n",
       "      <td>198</td>\n",
       "      <td>422</td>\n",
       "      <td>1246</td>\n",
       "      <td>0.750</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>326</td>\n",
       "      <td>50</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.669</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>299</td>\n",
       "      <td>526</td>\n",
       "      <td>1794</td>\n",
       "      <td>100</td>\n",
       "      <td>384</td>\n",
       "      <td>914</td>\n",
       "      <td>0.069</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>503</td>\n",
       "      <td>160</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.736</td>\n",
       "      <td>33</td>\n",
       "      <td>16</td>\n",
       "      <td>321</td>\n",
       "      <td>527</td>\n",
       "      <td>2369</td>\n",
       "      <td>157</td>\n",
       "      <td>444</td>\n",
       "      <td>1188</td>\n",
       "      <td>0.515</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ast  blk  fg3_pct  fg_pct  ft_pct   g  losses  orb   pf   pts  stl  tov  \\\n",
       "0  408   83    0.376   0.464   0.663  29      16  222  631  2009  210  412   \n",
       "1  494   57    0.363   0.436   0.717  33      21  304  594  2411  197  390   \n",
       "2  517  118    0.372   0.466   0.683  36       9  357  653  2772  198  422   \n",
       "3  326   50    0.280   0.405   0.669  29      27  299  526  1794  100  384   \n",
       "4  503  160    0.348   0.462   0.736  33      16  321  527  2369  157  444   \n",
       "\n",
       "    trb  win_loss_pct  wins  bias  \n",
       "0   904         0.448    13   1.0  \n",
       "1  1149         0.364    12   1.0  \n",
       "2  1246         0.750    27   1.0  \n",
       "3   914         0.069     2   1.0  \n",
       "4  1188         0.515    17   1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_data():\n",
    "    #CLEAN DATA\n",
    "    df = pd.read_csv(\"scraped_data_lr.csv\")\n",
    "    df.drop(['Unnamed: 0'], axis = 1, inplace=True)\n",
    "    #insert bias term\n",
    "    df['bias']=1.0\n",
    "    df.drop(['school_name'], axis = 1, inplace=True)\n",
    "    return df\n",
    "    \n",
    "X_train=process_data()\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ast</th>\n",
       "      <th>blk</th>\n",
       "      <th>fg3_pct</th>\n",
       "      <th>fg_pct</th>\n",
       "      <th>ft_pct</th>\n",
       "      <th>g</th>\n",
       "      <th>losses</th>\n",
       "      <th>orb</th>\n",
       "      <th>pf</th>\n",
       "      <th>pts</th>\n",
       "      <th>stl</th>\n",
       "      <th>tov</th>\n",
       "      <th>trb</th>\n",
       "      <th>win_loss_pct</th>\n",
       "      <th>wins</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>482</td>\n",
       "      <td>128</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.701</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>305</td>\n",
       "      <td>671</td>\n",
       "      <td>2359</td>\n",
       "      <td>260</td>\n",
       "      <td>461</td>\n",
       "      <td>1093</td>\n",
       "      <td>0.500</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>445</td>\n",
       "      <td>78</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.734</td>\n",
       "      <td>31</td>\n",
       "      <td>19</td>\n",
       "      <td>302</td>\n",
       "      <td>557</td>\n",
       "      <td>2124</td>\n",
       "      <td>201</td>\n",
       "      <td>391</td>\n",
       "      <td>1014</td>\n",
       "      <td>0.387</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>417</td>\n",
       "      <td>81</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.696</td>\n",
       "      <td>32</td>\n",
       "      <td>18</td>\n",
       "      <td>300</td>\n",
       "      <td>637</td>\n",
       "      <td>2296</td>\n",
       "      <td>189</td>\n",
       "      <td>440</td>\n",
       "      <td>1066</td>\n",
       "      <td>0.438</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>340</td>\n",
       "      <td>49</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.647</td>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "      <td>328</td>\n",
       "      <td>506</td>\n",
       "      <td>1873</td>\n",
       "      <td>123</td>\n",
       "      <td>514</td>\n",
       "      <td>1046</td>\n",
       "      <td>0.097</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>560</td>\n",
       "      <td>127</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.750</td>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>334</td>\n",
       "      <td>490</td>\n",
       "      <td>2536</td>\n",
       "      <td>178</td>\n",
       "      <td>437</td>\n",
       "      <td>1259</td>\n",
       "      <td>0.606</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ast  blk  fg3_pct  fg_pct  ft_pct   g  losses  orb   pf   pts  stl  tov  \\\n",
       "0  482  128    0.325   0.464   0.701  32      16  305  671  2359  260  461   \n",
       "1  445   78    0.331   0.419   0.734  31      19  302  557  2124  201  391   \n",
       "2  417   81    0.360   0.435   0.696  32      18  300  637  2296  189  440   \n",
       "3  340   49    0.303   0.397   0.647  31      28  328  506  1873  123  514   \n",
       "4  560  127    0.345   0.488   0.750  33      13  334  490  2536  178  437   \n",
       "\n",
       "    trb  win_loss_pct  wins  bias  \n",
       "0  1093         0.500    16   1.0  \n",
       "1  1014         0.387    12   1.0  \n",
       "2  1066         0.438    14   1.0  \n",
       "3  1046         0.097     3   1.0  \n",
       "4  1259         0.606    20   1.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_data_test_X():\n",
    "    #CLEAN DATA\n",
    "    df = pd.read_csv(\"scraped_data_lr_test_X.csv\")\n",
    "    df.drop(['Unnamed: 0'], axis = 1, inplace=True)\n",
    "    #insert bias term\n",
    "    df['bias']=1.0\n",
    "    df.drop(['school_name'], axis = 1, inplace=True)\n",
    "    return df\n",
    "    \n",
    "X_test=process_data_test_X()\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n",
      "351\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping The Output Data\n",
    "\n",
    "We also need to scrape the sports-reference website to get ratings to train on also test the accuracy of our model on.\n",
    "\n",
    "**2017 Ratings** for each school, provided by the website, will be used to train our model on\n",
    "\n",
    "**2018 Ratings** for each school, provided by the website, will be used to test the accuracy of our model on (by comparing it with the models predicted values for 2018).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2017 Rankings- Training Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Manini/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/Manini/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "def getY():\n",
    "    url = \"https://www.sports-reference.com/cbb/seasons/2017-school-stats.html\"\n",
    "    page = urlopen(url).read()\n",
    "    soup = BeautifulSoup(page)\n",
    "    count  = 0\n",
    "    table = soup.find(\"tbody\")\n",
    "    featuresWanted =  {'school_name', 'srs'}\n",
    "    rows = table.findChildren(['tr'])\n",
    "    pre_df={}\n",
    "    rows\n",
    "    for row in rows:\n",
    "        if (row.find('th', {\"scope\":\"row\"}) != None):\n",
    "            for f in featuresWanted:\n",
    "                cell = row.find(\"td\",{\"data-stat\": f})\n",
    "                a = cell.text.strip().encode()\n",
    "                text=a.decode(\"utf-8\")\n",
    "                if f in pre_df:\n",
    "                    pre_df[f].append(text)\n",
    "                else:\n",
    "                    pre_df[f]=[text]\n",
    "            df = pd.DataFrame.from_dict(pre_df)\n",
    "        \n",
    "    return df\n",
    "def csvDumpTrainY():\n",
    "    df=getY()\n",
    "    df.to_csv(\"scraped_data_lr_test.csv\")\n",
    "csvDumpTrainY()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018 Rankings- Testing Outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Manini/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/Manini/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "def getY2():\n",
    "    url = \"https://www.sports-reference.com/cbb/seasons/2018-school-stats.html\"\n",
    "    page = urlopen(url).read()\n",
    "    soup = BeautifulSoup(page)\n",
    "    count  = 0\n",
    "    table = soup.find(\"tbody\")\n",
    "    featuresWanted =  {'school_name', 'srs'}\n",
    "    rows = table.findChildren(['tr'])\n",
    "    pre_df={}\n",
    "    rows\n",
    "    for row in rows:\n",
    "        if (row.find('th', {\"scope\":\"row\"}) != None):\n",
    "            for f in featuresWanted:\n",
    "                cell = row.find(\"td\",{\"data-stat\": f})\n",
    "                a = cell.text.strip().encode()\n",
    "                text=a.decode(\"utf-8\")\n",
    "                if f in pre_df:\n",
    "                    pre_df[f].append(text)\n",
    "                else:\n",
    "                    pre_df[f]=[text]\n",
    "            \n",
    "        df = pd.DataFrame.from_dict(pre_df)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def csvDumpTestY():\n",
    "    df=getY2()\n",
    "    df.to_csv(\"scraped_data_lr_test_Y.csv\")\n",
    "csvDumpTestY()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and Validate Output Data\n",
    "\n",
    "Let's clean up and validate the output data by removing unwanted columns and making sure all schools have entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n",
      "351\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_train_Y():\n",
    "    #CLEAN DATA\n",
    "    df = pd.read_csv(\"scraped_data_lr_test.csv\")\n",
    "    df.drop(['Unnamed: 0'], axis = 1, inplace=True)\n",
    "    df.drop(['school_name'], axis = 1, inplace=True)\n",
    "\n",
    "    return df\n",
    "y_train=process_train_Y()\n",
    "def process_test_Y():\n",
    "    #CLEAN DATA\n",
    "    df = pd.read_csv(\"scraped_data_lr_test_Y.csv\")\n",
    "    df.drop(['Unnamed: 0'], axis = 1, inplace=True)\n",
    "    df.drop(['school_name'], axis = 1, inplace=True)\n",
    "\n",
    "    return df\n",
    "y_test=process_test_Y()\n",
    "print(y_train.shape[0])\n",
    "print(y_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating and Running the Linear Regression Model\n",
    "\n",
    "1.) Instantiate a linear regression model using the sklearn library\n",
    "\n",
    "2.) Fit the model to the training set (2016-2017 season data and rankings)\n",
    "\n",
    "3.) Predict 2018 rankings using 2017-2018 season data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.09145317]\n",
      " [ -5.95802877]\n",
      " [ -8.49269767]\n",
      " [-25.34304605]\n",
      " [  9.21925457]\n",
      " [-11.4941322 ]\n",
      " [  3.52412716]\n",
      " [  6.77605817]\n",
      " [ -9.13922422]\n",
      " [-13.35458591]\n",
      " [ -5.21331213]\n",
      " [  4.42626806]\n",
      " [ 15.68032268]\n",
      " [-14.14654717]\n",
      " [ -6.84019499]\n",
      " [ -8.26361591]\n",
      " [  9.85210845]\n",
      " [ -5.49807188]\n",
      " [ 11.35167106]\n",
      " [ -2.96448602]\n",
      " [  0.41556419]\n",
      " [  7.63803276]\n",
      " [  5.18995381]\n",
      " [ -3.84241557]\n",
      " [ -4.42008136]\n",
      " [  1.23663647]\n",
      " [  2.48517896]\n",
      " [ -3.65112866]\n",
      " [ -3.51563778]\n",
      " [  3.90264421]\n",
      " [ 10.77361847]\n",
      " [ -8.51729958]\n",
      " [-16.86553095]\n",
      " [  7.09303318]\n",
      " [ 10.61424246]\n",
      " [  6.79123599]\n",
      " [ -3.58630956]\n",
      " [ -7.78699831]\n",
      " [ -0.20774513]\n",
      " [ -9.27419088]\n",
      " [  0.07102235]\n",
      " [  4.31523565]\n",
      " [-10.18860112]\n",
      " [  9.83032832]\n",
      " [-10.70378091]\n",
      " [  5.71221975]\n",
      " [  1.72465963]\n",
      " [  3.96910872]\n",
      " [ -4.99775767]\n",
      " [ -4.17465712]\n",
      " [  6.18023904]\n",
      " [ -2.27236221]\n",
      " [-15.20434276]\n",
      " [-11.56826205]\n",
      " [-18.25929224]\n",
      " [ 22.55229686]\n",
      " [-11.51438168]\n",
      " [ 12.76656157]\n",
      " [ -8.98542462]\n",
      " [ -3.9514532 ]\n",
      " [  4.18297834]\n",
      " [  9.59043855]\n",
      " [ -6.18388038]\n",
      " [ -1.03404484]\n",
      " [ -4.5461269 ]\n",
      " [ -3.3322362 ]\n",
      " [-18.30734014]\n",
      " [ -4.2696315 ]\n",
      " [  7.2214063 ]\n",
      " [ -8.58362409]\n",
      " [ 10.05127999]\n",
      " [ -4.14599915]\n",
      " [-16.19572812]\n",
      " [ -3.23087271]\n",
      " [ -3.45099575]\n",
      " [ -5.81547639]\n",
      " [ -9.98177774]\n",
      " [  0.1777402 ]\n",
      " [ -1.58560159]\n",
      " [ 21.00696695]\n",
      " [  0.27388482]\n",
      " [-11.53288289]\n",
      " [ 10.13904464]\n",
      " [ -3.22669073]\n",
      " [ -4.08353708]\n",
      " [ 12.78612806]\n",
      " [  4.11598156]\n",
      " [ -3.72718814]\n",
      " [  2.28814736]\n",
      " [ -3.30347811]\n",
      " [ -4.42528477]\n",
      " [-11.22992703]\n",
      " [ -1.45039788]\n",
      " [  5.67575206]\n",
      " [ -4.77640304]\n",
      " [  9.15787093]\n",
      " [ 10.3094426 ]\n",
      " [-10.55738881]\n",
      " [  4.96315801]\n",
      " [  4.38186747]\n",
      " [ -3.74701404]\n",
      " [ -2.54616666]\n",
      " [  3.60406767]\n",
      " [ -0.33751178]\n",
      " [  1.10189371]\n",
      " [  9.42008994]\n",
      " [  1.76318731]\n",
      " [  3.60828772]\n",
      " [ 18.29093688]\n",
      " [ -5.97619211]\n",
      " [  4.50140396]\n",
      " [ -4.40799849]\n",
      " [  4.1290936 ]\n",
      " [  1.29576675]\n",
      " [  0.55455372]\n",
      " [ -7.30678509]\n",
      " [ -1.47203602]\n",
      " [  0.76652027]\n",
      " [ -3.48606238]\n",
      " [-13.89981132]\n",
      " [ 11.67810391]\n",
      " [-10.29076108]\n",
      " [ -7.786031  ]\n",
      " [  3.83922791]\n",
      " [  7.91069814]\n",
      " [ -1.52576287]\n",
      " [ -6.91853918]\n",
      " [ -9.27366132]\n",
      " [ -7.26381637]\n",
      " [  0.99377877]\n",
      " [  3.77302456]\n",
      " [  0.1594365 ]\n",
      " [  3.83242181]\n",
      " [  1.03253655]\n",
      " [ -8.44478436]\n",
      " [ -7.70013499]\n",
      " [ 12.02264978]\n",
      " [ -6.30904586]\n",
      " [ -4.46598723]\n",
      " [  8.08996894]\n",
      " [ 17.72207919]\n",
      " [ -5.91861659]\n",
      " [ -2.21462159]\n",
      " [ 12.28200627]\n",
      " [ -1.62494737]\n",
      " [ -4.54358502]\n",
      " [ -3.27547391]\n",
      " [ -1.32617911]\n",
      " [  8.36675584]\n",
      " [ -2.29813155]\n",
      " [ -4.31659197]\n",
      " [ -3.46801027]\n",
      " [-16.61171913]\n",
      " [ 12.51232393]\n",
      " [ -3.68089346]\n",
      " [  4.06432935]\n",
      " [  2.35303498]\n",
      " [ 14.74799599]\n",
      " [ 14.3962978 ]\n",
      " [ -6.41060041]\n",
      " [ -9.36067536]\n",
      " [-16.49508593]\n",
      " [ -4.83891676]\n",
      " [-17.41293212]\n",
      " [  7.24281387]\n",
      " [ 11.17160812]\n",
      " [  3.22490467]\n",
      " [-17.42770005]\n",
      " [  6.23687963]\n",
      " [ -7.12457684]\n",
      " [ -0.96145692]\n",
      " [ -4.87752383]\n",
      " [  0.62098806]\n",
      " [  1.95074905]\n",
      " [  5.85906806]\n",
      " [ -3.19385879]\n",
      " [ 27.83519806]\n",
      " [ 17.55594334]\n",
      " [  6.64570757]\n",
      " [ -0.05004409]\n",
      " [  5.75729735]\n",
      " [ 10.94287437]\n",
      " [-17.79807563]\n",
      " [  0.70981095]\n",
      " [-13.47123606]\n",
      " [  6.37423677]\n",
      " [  3.79329727]\n",
      " [ -9.40928771]\n",
      " [ -7.42913082]\n",
      " [  7.4837108 ]\n",
      " [-13.42964515]\n",
      " [ -5.72144075]\n",
      " [  2.63063338]\n",
      " [ 10.76176222]\n",
      " [ -4.11796005]\n",
      " [ -9.12566017]\n",
      " [ 10.88515676]\n",
      " [  4.66281838]\n",
      " [ 17.73519589]\n",
      " [ -6.95483171]\n",
      " [  7.35603195]\n",
      " [ -0.44514216]\n",
      " [ -2.76028006]\n",
      " [  3.37664135]\n",
      " [  0.76503325]\n",
      " [ -3.40754926]\n",
      " [ -6.097965  ]\n",
      " [  4.38375893]\n",
      " [ -2.22649797]\n",
      " [  2.70866702]\n",
      " [  7.74529695]\n",
      " [  5.75894343]\n",
      " [ -6.18587773]\n",
      " [ 17.57620748]\n",
      " [ -1.29795062]\n",
      " [-11.03344091]\n",
      " [ -4.40379167]\n",
      " [  0.12864787]\n",
      " [  3.10561834]\n",
      " [-19.13373503]\n",
      " [  5.38946351]\n",
      " [ -5.567398  ]\n",
      " [ -1.73427574]\n",
      " [  6.96057021]\n",
      " [-20.35872296]\n",
      " [  2.58172063]\n",
      " [ 12.54792878]\n",
      " [  7.25313385]\n",
      " [ 12.21619127]\n",
      " [ -3.31965377]\n",
      " [  6.051343  ]\n",
      " [  2.62911884]\n",
      " [ 13.89656814]\n",
      " [ -1.94115728]\n",
      " [  1.28103987]\n",
      " [ 10.28666225]\n",
      " [  2.22773325]\n",
      " [ 15.93787672]\n",
      " [  3.60341728]\n",
      " [ -7.52055584]\n",
      " [-12.02626237]\n",
      " [  2.71956453]\n",
      " [ -4.53491446]\n",
      " [ -6.44507504]\n",
      " [ -8.30940046]\n",
      " [ -3.60625192]\n",
      " [  3.72883842]\n",
      " [ 21.31468575]\n",
      " [ -9.99462842]\n",
      " [  4.99069706]\n",
      " [ 10.30695338]\n",
      " [-16.91717281]\n",
      " [ -3.31102245]\n",
      " [ -0.23229335]\n",
      " [ -9.3816367 ]\n",
      " [  0.22031495]\n",
      " [ -5.64319842]\n",
      " [ -9.95332986]\n",
      " [  0.54239944]\n",
      " [  0.5113417 ]\n",
      " [  0.5505821 ]\n",
      " [ 19.80268315]\n",
      " [ -4.23609901]\n",
      " [  0.04551409]\n",
      " [ -6.09638715]\n",
      " [  5.92834701]\n",
      " [  2.55276384]\n",
      " [  5.11449659]\n",
      " [-14.93510198]\n",
      " [ -6.89441248]\n",
      " [-12.45218905]\n",
      " [  1.11478537]\n",
      " [  6.1748686 ]\n",
      " [-12.09001541]\n",
      " [ -7.70514885]\n",
      " [-13.88173945]\n",
      " [ -8.00762513]\n",
      " [ -5.28057311]\n",
      " [ 10.25527616]\n",
      " [  8.07709981]\n",
      " [ -8.86666977]\n",
      " [ -5.6289101 ]\n",
      " [ -0.80703203]\n",
      " [ 12.69569031]\n",
      " [-11.78546902]\n",
      " [  3.66603327]\n",
      " [  3.40753452]\n",
      " [ -0.75960504]\n",
      " [ -6.97314475]\n",
      " [ -3.26666524]\n",
      " [ 10.10059029]\n",
      " [ -8.44654043]\n",
      " [  4.08293144]\n",
      " [ -0.84109187]\n",
      " [  4.29639206]\n",
      " [ -3.84011262]\n",
      " [ -7.50715596]\n",
      " [ 11.68033114]\n",
      " [  0.84388356]\n",
      " [ -5.72146698]\n",
      " [ -7.91224707]\n",
      " [ -2.9478419 ]\n",
      " [ 14.22029008]\n",
      " [-10.62990821]\n",
      " [ 11.50257835]\n",
      " [  2.41171033]\n",
      " [  8.89205079]\n",
      " [ -9.72840938]\n",
      " [ -4.22150031]\n",
      " [ -0.03475004]\n",
      " [ -0.82053717]\n",
      " [ -4.6698514 ]\n",
      " [ 12.55483147]\n",
      " [  3.34187283]\n",
      " [  9.46854909]\n",
      " [  4.46100281]\n",
      " [ -0.4098226 ]\n",
      " [ -1.14443852]\n",
      " [ -2.51706699]\n",
      " [  6.41289814]\n",
      " [ -5.02458197]\n",
      " [  9.9578718 ]\n",
      " [  7.56273782]\n",
      " [ -4.44218665]\n",
      " [ -2.80795073]\n",
      " [ 10.58481922]\n",
      " [ 23.99592826]\n",
      " [ -1.16514102]\n",
      " [-11.49498643]\n",
      " [  2.74755809]\n",
      " [ 22.1508526 ]\n",
      " [  4.19045047]\n",
      " [ -2.21000047]\n",
      " [ -7.61169239]\n",
      " [  3.42441066]\n",
      " [  6.98075993]\n",
      " [ 17.52526308]\n",
      " [ -9.78992262]\n",
      " [ -2.41549609]\n",
      " [ 13.07802762]\n",
      " [  2.05206799]\n",
      " [ 14.32154595]\n",
      " [  9.54154538]\n",
      " [  3.50057322]\n",
      " [  1.28635347]\n",
      " [  6.30311184]\n",
      " [  1.01455502]\n",
      " [ -1.40955174]\n",
      " [ 12.66344186]\n",
      " [ -0.9955786 ]\n",
      " [ -9.92424472]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Well is Our Model Performing?\n",
    "\n",
    "Let's compute the Mean Squared Error between the predicted rankings of all 351 schools from the LR model and\n",
    "the actual 2018 rankings provided by the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srs    30.254876\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def MSE(y_train,y_test):\n",
    "    return (np.square(y_train-y_test)).mean()\n",
    "\n",
    "print(MSE(y_pred, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and March Madness\n",
    "\n",
    "A MSE of 30.25 is comparable and even slightly better than the MSE that the ELO model produced!\n",
    "\n",
    "Now that we have verified the significance of the features we used in our model and the performance of our model,\n",
    "lets compare the rankings we produced for 2018 with actual March Madness rankings.\n",
    "\n",
    "\n",
    "\n",
    "In the code below, we will take the top 15 ranked teams after their 2017-2018 season and compare these top 15 teams with the top 15 teams from after March Madness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Michigan State\\xa0NCAA', 27.835198061156461), ('Villanova\\xa0NCAA', 23.995928255291943), ('Cincinnati\\xa0NCAA', 22.552296855649431), ('Virginia\\xa0NCAA', 22.150852600143878), ('Purdue\\xa0NCAA', 21.314685745185486), ('Duke\\xa0NCAA', 21.006966947664694), (\"Saint Mary's (CA)\\xa0NCAA\", 19.802683153474376), ('Gonzaga\\xa0NCAA', 18.290936880472472), ('Nevada\\xa0NCAA', 17.735195888947914), ('Kansas\\xa0NCAA', 17.722079192694849), ('North Carolina\\xa0NCAA', 17.576207482948874), ('Michigan\\xa0NCAA', 17.555943336738395), ('West Virginia\\xa0NCAA', 17.525263082362443), ('Penn State', 15.937876721879945), ('Arizona\\xa0NCAA', 15.680322679394024), ('Louisville\\xa0NCAA', 14.747995986727062)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import *\n",
    "\n",
    "#get a list of all schools whose indices map to their rankings\n",
    "def get_all_schools():\n",
    "    #CLEAN DATA\n",
    "    df = pd.read_csv(\"scraped_data_lr_test.csv\")\n",
    "    \n",
    "    return df['school_name']\n",
    "\n",
    "schools=get_all_schools()\n",
    "school=schools.tolist()\n",
    "\n",
    "#Rankings produced by our Model\n",
    "\n",
    "sub=list(chain.from_iterable(y_pred))\n",
    "#zip together schools with their rankings\n",
    "new=list(zip(school,sub))\n",
    "#sort list based on rankings\n",
    "new2=sorted(new, key=lambda x: x[1])\n",
    "\n",
    "new3=new2[::-1]\n",
    "#produce top 15 teams\n",
    "new4=new3[:16]\n",
    "\n",
    "x_pred_new= [x[0] for x in new4]\n",
    "y_pred_new=[x[1] for x in new4]\n",
    "final=list(zip(x_pred_new,y_pred_new))\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](lr.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
